{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import configparser\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "from scipy.io.wavfile import write\n",
    "import sys\n",
    "sys.path.append(\"../../hifi-gan/\")\n",
    "from env import AttrDict\n",
    "from meldataset import MAX_WAV_VALUE, MelDataset, get_dataset_filelist, mel_spectrogram\n",
    "from models import Generator\n",
    "from train import load_checkpoint, scan_checkpoint\n",
    "\n",
    "h = None\n",
    "device = None\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from env import AttrDict, build_env\n",
    "from meldataset import MelDataset, mel_spectrogram, get_dataset_filelist\n",
    "from models import Generator, MultiPeriodDiscriminator, MultiScaleDiscriminator, feature_loss, generator_loss,\\\n",
    "    discriminator_loss\n",
    "from utils import plot_spectrogram, scan_checkpoint, load_checkpoint, save_checkpoint\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "sys.path.append(\"../../cuhksz-phd/sho_util/pyfiles/\")\n",
    "from pytorch import cuda2numpy\n",
    "from basic import plot_spectrogram\n",
    "from sound import play_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = configparser.ConfigParser()\n",
    "base_dir = \"../\"\n",
    "data_dir = f\"{base_dir}LibriTTS/\"\n",
    "############ Checkpoint ###########\n",
    "# a.checkpoint_file = f'/mntcephfs/data/audiow/shoinoue/Model/models/hifigan/LibriTTS/g_00025000'\n",
    "# a.checkpoint_file = f'/mntcephfs/data/audiow/shoinoue/Model/models/hifigan/LibriTTS2/g_00031000'\n",
    "# a.checkpoint_file = f'/mntcephfs/data/audiow/shoinoue/Model/models/hifigan/LibriTTS3/g_00220000'\n",
    "a.checkpoint_file = f'/mntcephfs/data/audiow/shoinoue/Model/models/hifigan/LibriTTS4/g_00150000'\n",
    "###################################\n",
    "\n",
    "a.input_wavs_dir = \"/mntcephfs/\" \n",
    "a.input_mels_dir = \"/mntcephfs/\"\n",
    "a.input_training_file = f'{data_dir}/training.txt'\n",
    "a.input_validation_file = f'{data_dir}/validation.txt'\n",
    "\n",
    "a.fine_tuning = False\n",
    "\n",
    "scaler = None\n",
    "try:\n",
    "    # if int(os.path.dirname(a.checkpoint_file)[-1]) in [4, 5, 6]:\n",
    "    import joblib\n",
    "    scaler_filename = f\"../../seq2seq-vc/notebooks/ckpts/scalers/LibriTTS-R_hifiganmel.save\"\n",
    "    scaler = joblib.load(scaler_filename)\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "config_file = os.path.join(os.path.split(a.checkpoint_file)[0], 'config.json')\n",
    "with open(config_file) as f:\n",
    "    data = f.read()\n",
    "global h\n",
    "json_config = json.loads(data)\n",
    "h = AttrDict(json_config)\n",
    "\n",
    "torch.manual_seed(h.seed)\n",
    "global device\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(h.seed)\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "training_filelist, validation_filelist = get_dataset_filelist(a)\n",
    "validset = MelDataset(validation_filelist, h.segment_size, h.n_fft, h.num_mels,\n",
    "# validset = MelDataset(training_filelist, h.segment_size, h.n_fft, h.num_mels,\n",
    "                      h.hop_size, h.win_size, h.sampling_rate, h.fmin, h.fmax, False, False, n_cache_reuse=0,\n",
    "                      fmax_loss=h.fmax_for_loss, device=device, fine_tuning=a.fine_tuning,\n",
    "                      base_mels_path=a.input_mels_dir,\n",
    "                      scaler=scaler,\n",
    "                     )\n",
    "\n",
    "generator = Generator(h).to(device)\n",
    "state_dict_g = load_checkpoint(a.checkpoint_file, device)\n",
    "generator.load_state_dict(state_dict_g['generator'])\n",
    "generator.eval()\n",
    "generator.remove_weight_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x):\n",
    "    mean_ = torch.tensor(scaler.mean_.reshape(1, -1, 1)).to(x.device)\n",
    "    scale_ = torch.tensor(scaler.scale_.reshape(1, -1, 1)).to(x.device)\n",
    "    return (x-mean_)/scale_\n",
    "\n",
    "def inverse_transform(x):\n",
    "    mean_ = torch.tensor(scaler.mean_.reshape(1, -1, 1)).to(x.device)\n",
    "    scale_ = torch.tensor(scaler.scale_.reshape(1, -1, 1)).to(x.device)\n",
    "    return (x*scale_+mean_)\n",
    "\n",
    "def generate_audio(mel):\n",
    "    with torch.no_grad():\n",
    "        x = mel.unsqueeze(0).to(device)\n",
    "        audio = generator(x)\n",
    "        mel = mel_spectrogram(audio.squeeze(1), h.n_fft, h.num_mels, h.sampling_rate,\n",
    "                                      h.hop_size, h.win_size,\n",
    "                                      h.fmin, h.fmax_for_loss)\n",
    "        # if scaler!=None:\n",
    "            # mel = transform(mel)\n",
    "        audio = audio.squeeze()\n",
    "        audio = audio * MAX_WAV_VALUE\n",
    "        audio = audio.cpu().numpy().astype('int16')\n",
    "    return audio, cuda2numpy(mel)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "num = 5\n",
    "width = 2\n",
    "length = 1\n",
    "for i in range(num):\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    x, y, _, _ = validset[int(len(validset)/500)+i]\n",
    "    audio, mel = generate_audio(x)\n",
    "    fig = plt.figure(figsize=(4*width, 3*length))\n",
    "    if scaler!=None:\n",
    "        x = inverse_transform(x.unsqueeze(0))[0]\n",
    "    M = cuda2numpy(x)\n",
    "    ax = plot_spectrogram(M, fig, (length, width, 1), title=\"source\", title_font=12)\n",
    "    M = mel\n",
    "    ax = plot_spectrogram(M, fig, (length, width, 2), title=\"target\", title_font=12)\n",
    "    plt.show()\n",
    "    \n",
    "    play_audio(cuda2numpy(y), h.sampling_rate)\n",
    "    play_audio(audio, h.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "- Specific Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "# wavs_dir = \"/mntcephfs/lee_dataset/tts/LibriTTS_R/\" \n",
    "wavs_dir = \"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/English/wav/\"\n",
    "# mels_dir = \"/mntcephfs/data/audiow/shoinoue/Dataset/LibriTTS_R/features/\"\n",
    "mels_dir = \"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/English/hifiganmel/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(wavs_dir + \"*.wav\")\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "num = 3\n",
    "width = 2\n",
    "length = 1\n",
    "for i in range(num):\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    wavfile = files[i]\n",
    "    wav = librosa.load(wavfile, sr=h.sampling_rate)[0]\n",
    "    # melfile = mels_dir + wavfile[len(wavs_dir):-4] + \"_hifiganmel.npy\"\n",
    "    melfile = mels_dir + wavfile[len(wavs_dir):-4] + \".npy\"\n",
    "    melarray = torch.from_numpy(np.load(melfile))\n",
    "    if scaler!=None:\n",
    "        melarray = transform(melarray.unsqueeze(0))[0].to(melarray.dtype)\n",
    "    audio, mel = generate_audio(melarray)\n",
    "    fig = plt.figure(figsize=(4*width, 3*length))\n",
    "    M = cuda2numpy(inverse_transform(melarray.unsqueeze(0))[0])\n",
    "    ax = plot_spectrogram(M, fig, (length, width, 1), title=\"source\", title_font=12)\n",
    "    M = mel\n",
    "    ax = plot_spectrogram(M, fig, (length, width, 2), title=\"target\", title_font=12)\n",
    "    plt.show()\n",
    "    \n",
    "    play_audio(wav, h.sampling_rate)\n",
    "    play_audio(audio, h.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
